==================
Today's Bottleneck
==================

.. post:: 2013-01-04
   :author: tychoish
   :tags: cyborg, technology, hardware, software

Computers are always getting faster. From the perspective of the casual
observer it may seem like every year all of the various specs keep going
up, and systems are faster. [1]_ In truth, progress isn't uniform across
all systems and subsystems, and thinking about this progression of
technology gives us a chance to think about the constraints that
developers [2]_ and other people who build technology face.

For most of the past year, I've used a single laptop, for all of my
computing work, and while it's been great, in this time I lost touch
with the comparative speed of systems. No great loss, but I found myself
surprised to learn that all computers did not have the same speed: It
wasn't until I started using other machines on a regular basis that I
remembered that hardware could affect performance.

For most of the past decade, processors have been fast. While some
processors are theoretically faster and some have other features like
virtualization extensions and better multitasking capacities (i.e.
hyperthreading and multi-core systems) the improvements have been
incremental at best.

Memory (RAM) manages to mostly keep up with the processors, so there's
no real bottleneck between RAM and the processor. Although RAM
capacities are growing, at current volumes extra RAM just means
services/systems that *had* to be distributed given RAM density can all
run on one server. In general: "ho hum."

Disks are another story all together.

While disks got faster over this period, they didn't get *much* faster
during this period, and so for a long time disks were *the* bottle neck
in computing speed. To address this problem, a number of things changed:

-  We designed systems for asynchronous operation.

Basically, folks spilled a lot of blood and energy to make sure that
systems could continue to do work while waiting for the disk to reading
or writing data. This involves using a lot of event loops, queuing
systems, and so forth.

These systems are *really* cool, the only problem is that it means that
we have to be smarter about some aspects of software design and
deployment. This doesn't fix the *tons* of legacy sitting around, or the
fact that a lot of tools and programmers are struggling to keep up.

-  We started to build more distributed systems so that any individual
spinning disk is responsible for writing/reading less data.

-  We hacked disks themselves to get better performance.

There are some ways you can eek out a bit of extra performance from
spinning disks: namely RAID-10, hardware RAID controllers, and using
smaller platters. RAID approaches use multiple drives (4) to provide
simple redundancy and roughly double performance. Smaller platters
require less movement of the disk arm, and you get a bit more out of the
hardware.

Now, with affordable solid state disks (SSDs,) all of these disk related
speed problems are basically moot. So what are the next bottlenecks for
computers and performance:

-  Processors. It might be the case that processors are going to be the
slow to develop bottleneck. There are a lot of expectations on
processors these days: high speed, low power consumption, low
temperature, high amount of parallelism (cores and hyperthreading.)
But these expectations are necessarily conflicting.

The main route to innovation is to make the processors themselves
smaller, which *does* increase performance and helps control heat and
power consumption, but there is a practical limit to the size of a
processor.

Also, no matter how fast you make the processor, it's irrelevant unless
the software is capable of taking advantage of the feature.

-  Software.

We're still not *great* at building software with asynchronous
components. "Non-blocking" systems do make it easier to have systems
that work better with slower disks. Still, we don't have a lot of
software that does a great job of using the parallelism of a processor,
so it's possible to get some operations that are slow and will remain
slow because a single threaded process *must* grind through a long task
and can't share it.

-  Network overhead.

While I think better software is a huge problem, network throughput
could be a huge issue. The internet endpoints (your connection) has
gotten much faster in the past few years. That's a good thing, indeed,
but there are a number of problems:

-  Transfer speeds aren't keeping up with data growth or data storage,
and if that trend continues, we're going to end up with a lot of data
that only exists in one physical location, which leads to
catastrophic data loss.

I think we'll get back to a point where moving physical media around
will begin to make sense. Again.

-  Wireless data speeds and architectures (particularly 802.11x, but
also wide area wireless,) have become ubiquitous, but aren't really
sufficient for serious use. The fact that our homes, public places,
and even offices (in some cases) aren't wired correctly to be able to
provide opportunities to plug in will begin to hurt.

Thoughts? Other bottlenecks? Different reading of the history?

.. [1]
By contrast, software seems like its always getting slower, and while
this is *partially* true, there are additional factors at play,
including feature growth, programmer efficiency, and legacy support
requirements.

.. [2]
Because developers control, at least to some extent, how everyone
uses and understands technology, the constrains on the way they use
computers id important to everyone.

